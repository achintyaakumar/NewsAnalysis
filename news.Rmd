---
title: "News Analysis"
output: html_notebook
---
Install the newsanchor package using "install.packages("newsanchor")"
```{r}
library(newsanchor)
```
Make sure you've saved the api key before you move on.
"Sys.setenv(NEWS_API_KEY="Your_Key")"
```{r}
# get headlines for today
results_us_22 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
results_in_22 <- get_headlines_all(country = "in", api_key = Sys.getenv("NEWS_API_KEY"))
```
Sentiment analysis libraries
install.packages("textdata")
```{r}
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
```

```{r}
results_group <- results_in_14
results_group %>% group_by(results_in_14$results_df$name) %>% count() 
```
TEXT ANALYSIS
```{r}
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow", 
         main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
         xlab = "Freq")
```

```{r}
## NOUNS
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")
```

```{r}
## ADJECTIVES
stats <- subset(x, upos %in% c("ADJ")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "purple", 
         main = "Most occurring adjectives", xlab = "Freq")
```
```{r}
## NOUNS
stats <- subset(x, upos %in% c("VERB")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold", 
         main = "Most occurring Verbs", xlab = "Freq")
```
RAKE is one of the most popular (unsupervised) algorithms for extracting keywords in Information retrieval. RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurrence with other words in the text.
```{r}
## Using RAKE
library(udpipe)
library(lattice)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
s <- udpipe_annotate(udmodel_english, results_in_14$results_df$description)
x <- data.frame(s)
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id", 
                       relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red", 
         main = "Keywords identified by RAKE", 
         xlab = "Rake")
```

In English (or probably in many languages), Simple a noun and a verb can form a phrase. Like, Dog barked â€” with the noun Dog and Barked, we can understand the context of the sentence. Reverse-engineering the same with this headlines data, let us bring out top phrases - that are just keywords/topics
```{r}
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)
#stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")
```

TWITTER. 
setup_twitter_oauth("API key", "API secret")
```{r}
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)

twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=2, lang="en")

# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
```

```{r}
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)

# define "tolower error handling" function 
try.error = function(x)
{
   # create missing value
   y = NA
   # tryCatch error
   try_error = tryCatch(tolower(x), error=function(e) e)
   # if not an error
   if (!inherits(try_error, "error"))
   y = tolower(x)
   # result
   return(y)
}
# lower case using try.error with sapply 
some_txt = sapply(some_txt, try.error)

# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
```
```{r}
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
sentiments_twitter_i <- data.frame(syu, bing, afinn, nrc)
```

```{r}


```