install.packages("lattice", .Library)
## Using RAKE
library(udpipe)
library(lattice)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
s <- udpipe_annotate(udmodel_english, results_in_14$results_df$description)
x <- data.frame(s)
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
## Using RAKE
library(udpipe)
library(lattice)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
s <- udpipe_annotate(udmodel_english, results_in_14$results_df$description)
x <- data.frame(s)
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta",
main = "Keywords - simple noun phrases", xlab = "Frequency")
View(x)
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
## NOUNS
stats <- subset(x, upos %in% c("NOUN"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue",
main = "Most occurring nouns", xlab = "Freq")
## ADJECTIVES
stats <- subset(x, upos %in% c("ADJ"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "purple",
main = "Most occurring adjectives", xlab = "Freq")
## NOUNS
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs", xlab = "Freq")
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta",
main = "Keywords - simple noun phrases", xlab = "Frequency")
## Using RAKE
library(udpipe)
library(lattice)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
s <- udpipe_annotate(udmodel_english, results_in_14$results_df$description)
x <- data.frame(s)
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta",
main = "Keywords - simple noun phrases", xlab = "Frequency")
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
#stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta",
main = "Keywords - simple noun phrases", xlab = "Frequency")
udmodel_english <- udpipe_load_model(file = 'english-ud-2.0-170801.udpipe')
install.packages("twitteR")
library(twitteR)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=1500, lang="en")
setup_twitter_oauth("
PDq29zzxqi5UzBLx34uYy25Yd", "
maKK72rk85F7HoyO15n3PkZn0zZfa1Ap8JbwaQ6ASmiBjz4EPU")
setup_twitter_oauth("PDq29zzxqi5UzBLx34uYy25Yd", "maKK72rk85F7HoyO15n3PkZn0zZfa1Ap8JbwaQ6ASmiBjz4EPU")
create_token(app=NewsAnalysisR,consumer_key="
PDq29zzxqi5UzBLx34uYy25Yd", consumer_secret="maKK72rk85F7HoyO15n3PkZn0zZfa1Ap8JbwaQ6ASmiBjz4EPU", access_token="738337291408543744-h3VTzbsgDtnI8zunIxLOSXe2XgQ7QHi", access_secret="sHyxJgQAihPEoXz7SBskMeCik42Vj04IZiugJ6w2l6ZRS")
setup_twitter_oauth("PDq29zzxqi5UzBLx34uYy25Yd", "maKK72rk85F7HoyO15n3PkZn0zZfa1Ap8JbwaQ6ASmiBjz4EPU")
install.packages("openssl")
install.packages("httpuv")
install.packages("openssl")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa")
devtools::install_github("mkearney/rtweet")
install.packages("httk")
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=1500, lang="en")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "
+ 7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa")
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
appname <- "NewsAnalysisR"
key <- "Z19KgKizE9OlZHDOphLINua7m"
secret <- "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=1500, lang="en")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa", "738337291408543744-jmEkXOookBAmx9QmA0N6bzti6KNbuSA", "vENOSrQWpN9RMbAyQJqgY3Qg7AvaYn0BstE2HdkrpePmq")
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
appname <- "NewsAnalysisR"
key <- "Z19KgKizE9OlZHDOphLINua7m"
secret <- "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=2, lang="en")
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
View(some_tweets)
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
library(sentiment)
install.packages("SentimentAnalysis")
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
library(sentiment)
install.packages(c("NLP","tm","Rstem"))
library(NLP)
library(sentiment)
install.packages("pacman")
if (!require('pacman')) install.packages('pacman&')
pacman::p_load(devtools, installr)
install.Rtools()
install.packages(rtools)
# get headlines for today
results_us_22 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
# get headlines for today
results_us_22 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
Sys.setenv(NEWS_API_KEY="2ab94d79bd574e63a0c6b6223446b370")
# get headlines for today
results_us_22 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
library(newsanchor)
# get headlines for today
results_us_22 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
results_in_22 <- get_headlines_all(country = "in", api_key = Sys.getenv("NEWS_API_KEY"))
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
if (!require('pacman')) install.packages('pacman&')
pacman::p_load(devtools, installr)
install.Rtools()
install.packages("C:\Users\Achintya\Downloads\cmder\NewsAnalysis\sentiment_0.2.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/Achintya/Downloads/cmder/NewsAnalysis/sentiment_0.2.tar.gz", repos = NULL, type = "source")
library(NLP)
library(sentiment)
install.Rtools(page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/")
install.rtools()
library(syuzhet)
install.packages("syuzhet")
library(syuzhet)
some_txt
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
sentiments_twitter_i <- data.frame(syu, bing, afinn, nrc, timestamp
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
sentiments_twitter_i <- data.frame(syu, bing, afinn, nrc, timestamp)
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
sentiments_twitter_i <- data.frame(syu, bing, afinn, nrc)
View(sentiments_twitter_i)
Sys.setenv(NEWS_API_KEY="2ab94d79bd574e63a0c6b6223446b370")
library(newsanchor)
# get headlines for today
results_us_26 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
results_in_26 <- get_headlines_all(country = "in", api_key = Sys.getenv("NEWS_API_KEY"))
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=100, lang="en")
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
appname <- "NewsAnalysisR"
key <- "Z19KgKizE9OlZHDOphLINua7m"
secret <- "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=100, lang="en")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa")
setup_twitter_oauth(Z19KgKizE9OlZHDOphLINua7m, 7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa)
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
appname <- "NewsAnalysisR"
key <- "Z19KgKizE9OlZHDOphLINua7m"
secret <- "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=100, lang="en")
devtools::install_github("mkearney/rtweet")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa", "738337291408543744-jmEkXOookBAmx9QmA0N6bzti6KNbuSA", "vENOSrQWpN9RMbAyQJqgY3Qg7AvaYn0BstE2HdkrpePmq")
setup_twitter_oauth("Z19KgKizE9OlZHDOphLINua7m", "7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa", "738337291408543744-pQaLxKx5a3WJi2qtIs9UnYjacz2NTuV", "gX9RrjBW8PYhV6EyxUSUQj1f3s4wEng4YLp13MKKJl5V7")
library(twitteR)
library(devtools)
library(rtweet)
library(httk)
library(httr)
appname <- "NewsAnalysisR"
key <- "Z19KgKizE9OlZHDOphLINua7m"
secret <- "
7ZkuB5tHkZyEy3HS8uAAZ7qgsv3aYk1ECp3tIjZHtaoZouRVEa"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# harvest some tweets
some_tweets = searchTwitter("starbucks", n=100, lang="en")
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
library(syuzhet)
syu <- get_sentiment(some_txt, method = "syuzhet")
bing <- get_sentiment(some_txt, method = "bing")
afinn <- get_sentiment(some_txt, method = "afinn")
nrc <- get_sentiment(some_txt, method = "nrc")
sentiments_twitter_i <- data.frame(syu, bing, afinn, nrc)
View(sentiments_twitter_i)
#model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
model <- udpipe_download_model(language = "english")
library(udpipe)
library(lattice)
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model$file_model)
s <- udpipe_annotate(udmodel_english, results_us_14$results_df$description)
x <- data.frame(s)
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
## NOUNS
stats <- subset(x, upos %in% c("NOUN"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue",
main = "Most occurring nouns", xlab = "Freq")
## ADJECTIVES
stats <- subset(x, upos %in% c("ADJ"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "purple",
main = "Most occurring adjectives", xlab = "Freq")
## NOUNS
stats <- subset(x, upos %in% c("VERB"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "gold",
main = "Most occurring Verbs", xlab = "Freq")
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 10), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 1), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 0), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using RAKE
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id",
relevant = x$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 1), 20), col = "red",
main = "Keywords identified by RAKE",
xlab = "Rake")
## Using a sequence of POS tags (noun phrases / verb phrases)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
#stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta",
main = "Keywords - simple noun phrases", xlab = "Frequency")
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/entity")
library(entity)
library(newsanchor)
# get headlines for today
results_us_26 <- get_headlines_all(country = "us", api_key = Sys.getenv("NEWS_API_KEY"))
results_in_26 <- get_headlines_all(country = "in", api_key = Sys.getenv("NEWS_API_KEY"))
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
library(ggplot2)
results_group <- results_in_14
results_group %>% group_by(results_in_14$results_df$name) %>% count()
